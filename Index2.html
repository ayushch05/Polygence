# Ayush's Amazan Dataset Predictions and Improving Customer Service Through Suggestions
My project is to build a smart car with a self-cooling system and also a robotic arm to pick up and carry objects. The robotic arm smart car has a fan that turns on/off based on the temperature measured by the thermostat assembled on the smart car. My project also has infrared sensors and a thermostat, each for different functionalities. The infrared sensors are placed underneath the rover to detect ground surface within a 5-centimeter range by emitting light waves and receiving reflected light waves. The thermostat's function is to measure the room temperature, and based on a certain room temperature, the thermostat sends a signal to arduino controller board to either turn on or off the fan. 

| **Engineer** | **School** | **Area of Interest** | **Grade** |
|:--:|:--:|:--:|:--:|
| Ayush C | Mountain House High School | Mechatronic Engineering | Senior

![Headstone Image](IMG-1417.jpg)
  
# Introduction
“Machine intelligence is the last invention that humanity will ever need to make.” This concept known as Artificial Intelligence uses human aspects like machine learning, speech recognition, and visual perception to accelerate the progress of this world. To give an insight into machine learning, the best and optimal models are being created to shape the patterns and trends of datasets. Most datasets have a lot of outliers and raw data, but they also contain data with the potential of having meaningful insights. In order to extract meaningful insights, it is very important to fit the model correctly without underfitting or overfitting the dataset. 

Some models can have flaws of either underfitting or overfitting the data, therefore not generalizing patterns in the data leading to bias in the model. Overfitting is a common pitfall in deep learning algorithms, in which a model tries to fit the training data entirely and ends up memorizing the data patterns and the noise/random fluctuations. An overfitting-model can work or perform extremely and as close as to a 100%-accuracy. This indicates that this model trains way too much with one dataset and memorizes all of the features of that particular dataset, leading to the model’s massive inefficiency with other varieties of different datasets. On the other hand, underfitting is another common pitfall in machine learning by being unable to map between the input and the target variable. Under-observing the features leads to a higher error in the training and unseen data samples. It is different from overfitting, where the model performs well in the training set but fails to generalize the learning to the testing set ( Baheti). In this case, an underfitting-data model is very simple and is unable to create a relationship between the input and output variables by under-observing any trends and patterns leading to a high error while training the data.

 As shown in the image above, an overfitted model seems to be close to perfect in accuracy rate as the model over-observes and memorizes every feature in the dataset making it a bad model to generalize other datasets. Under Fitted models cannot exactly fit the data under the line and are unable to create a strong relationship to separate the data values. In this case, a linear model is a good example of an underfitting model as it fails to create a strong predictive model. A good fitting model that can generalize every dataset consistently well without any bias is shown as it nicely draws a relationship for regression purposes (Baheti). 

K-fold cross-validation is a technique in machine learning to test the accuracy of the model and prevent overfitting. Cross validation splits a dataset into K separate chunks, and then prediction is performed via leave-one-out. Essentially, we will take 1 chunk of data for testing and the remaining K-1 chunks of data for training. We will repeat this K number of times to get an overall idea of how our model performs (Baheti). 

- <img src="IMG-1407 (1).jpg" alt="Assembled Rover" width="200" height="250">
-  <img src="1CA60666-985A-4DFE-B3C1-500DFDA6B598.jpeg" alt="Assembled Rover" width="330" height="250">
- <img src="91E2CE97-54B5-4208-A8AB-7262AFB2E3F6.jpeg" alt="Assembled Rover" width="730" height="500">
<iframe width="560" height="315" src="https://www.youtube.com/embed/fvaJicvpfIg" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>

# Labeled Vs Unlabeled Data
- On the other hand, it is also important to know and recognize another machine learning method that deals with unlabeled data. Some examples include images, audio recordings, and medical scan reports. 
- As we saw before, models are optimized to reduce error given a label. It is very much possible to do this task with the help of a method called unsupervised learning. 
- Unsupervised learning uses machine learning algorithms to analyze and cluster unlabeled datasets. 
- The purpose of this model is to discover any hidden trends or patterns, mainly similarities and differences among the dataset through clustering methods.
- Grouping the datasets into different clusters based on similar features can be done in a variety of methods. One way to use unsupervised learning is KMeans that separates a dataset into k clusters based on k centroids of data. 
- The algorithm measures the average distance of every point from the centroids and groups the points closest to each centroid creating k clusters. 

- <img src="IMG-1404.jpg" alt="Assembled Rover" width="500" height="400">
- <img src="IMG-1406.jpg" alt="Assembled Rover" width="500" height="400">
- <iframe width="560" height="315" src="https://www.youtube.com/embed/X3QUw-6nA50" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>

# Testing Models on California Housing & FIFA 2023 Cards Datasets
I initially trained some models on dataframes of California housing prices. These are the steps I took to training my models: 
I tried the model of linear regression by using  mean squared error between the housing values predicted by the regression line and the actual and observed housing values. 
These are the steps I took to training my models: 
1. First, my mentor and I made a regression line between x and y variables. 
2. We decided to make every feature (longitude, latitude, housing median age, total number of rooms, total number of bedrooms, population, households, and median income) the explanatory or independent variable while we made the median house value the response variable. 
3. From the regression line, we can understand the relationship between the changes in the x variables and the changes in housing values. 
4. We first made individual arrays for the x and y variables. 
5. Lastly, we made a function to estimate the coefficient of slope and the y-intercept for the regression line.
- <img src="IMG-1373.jpg" alt="Assembled Rover" width="500" height="400">
- In order to test my code, I would use an USB adapter to upload my code into the arduino board.
- <img src="IMG-1374.jpg" alt="" width="500" height="400">

#Amazon Customer Reviews Dataset

- <img src="Arduino-UNO-Description.png" alt="Arduino board" width="500" height="400">
- <img src="H-Bridge-Motor-Driver-Circuit-using-L293D.jpeg" alt="H Bridge" width="500" height="400">
<iframe width="560" height="315" src="https://www.youtube.com/embed/DG3XcSVxVLQ" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>

#Amazon Customer Reviews Dataset

# Schematics 
![Headstone Image](5A416919-024C-4747-82ED-2BB453C713FC.jpeg) 


