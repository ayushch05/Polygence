# Ayush's Amazan Dataset Predictions and Improving Customer Service Through Suggestions
My project is to build a smart car with a self-cooling system and also a robotic arm to pick up and carry objects. The robotic arm smart car has a fan that turns on/off based on the temperature measured by the thermostat assembled on the smart car. My project also has infrared sensors and a thermostat, each for different functionalities. The infrared sensors are placed underneath the rover to detect ground surface within a 5-centimeter range by emitting light waves and receiving reflected light waves. The thermostat's function is to measure the room temperature, and based on a certain room temperature, the thermostat sends a signal to arduino controller board to either turn on or off the fan. 

| **Engineer** | **School** | **Area of Interest** | **Grade** |
|:--:|:--:|:--:|:--:|
| Ayush C | Mountain House High School | Mechatronic Engineering | Senior

![Headstone Image](IMG-1417.jpg)
  
# Introduction
“Machine intelligence is the last invention that humanity will ever need to make.” This concept known as Artificial Intelligence uses human aspects like machine learning, speech recognition, and visual perception to accelerate the progress of this world. To give an insight into machine learning, the best and optimal models are being created to shape the patterns and trends of datasets. Most datasets have a lot of outliers and raw data, but they also contain data with the potential of having meaningful insights. In order to extract meaningful insights, it is very important to fit the model correctly without underfitting or overfitting the dataset. 

Some models can have flaws of either underfitting or overfitting the data, therefore not generalizing patterns in the data leading to bias in the model. Overfitting is a common pitfall in deep learning algorithms, in which a model tries to fit the training data entirely and ends up memorizing the data patterns and the noise/random fluctuations. An overfitting-model can work or perform extremely and as close as to a 100%-accuracy. This indicates that this model trains way too much with one dataset and memorizes all of the features of that particular dataset, leading to the model’s massive inefficiency with other varieties of different datasets. On the other hand, underfitting is another common pitfall in machine learning by being unable to map between the input and the target variable. Under-observing the features leads to a higher error in the training and unseen data samples. It is different from overfitting, where the model performs well in the training set but fails to generalize the learning to the testing set ( Baheti). In this case, an underfitting-data model is very simple and is unable to create a relationship between the input and output variables by under-observing any trends and patterns leading to a high error while training the data.

 As shown in the image above, an overfitted model seems to be close to perfect in accuracy rate as the model over-observes and memorizes every feature in the dataset making it a bad model to generalize other datasets. Under Fitted models cannot exactly fit the data under the line and are unable to create a strong relationship to separate the data values. In this case, a linear model is a good example of an underfitting model as it fails to create a strong predictive model. A good fitting model that can generalize every dataset consistently well without any bias is shown as it nicely draws a relationship for regression purposes (Baheti). 

K-fold cross-validation is a technique in machine learning to test the accuracy of the model and prevent overfitting. Cross validation splits a dataset into K separate chunks, and then prediction is performed via leave-one-out. Essentially, we will take 1 chunk of data for testing and the remaining K-1 chunks of data for training. We will repeat this K number of times to get an overall idea of how our model performs (Baheti). 

- <img src="Fitting.png">
-  <img src="1CA60666-985A-4DFE-B3C1-500DFDA6B598.jpeg" alt="Assembled Rover" width="330" height="250">
- <img src="91E2CE97-54B5-4208-A8AB-7262AFB2E3F6.jpeg" alt="Assembled Rover" width="730" height="500">
<iframe width="560" height="315" src="https://www.youtube.com/embed/fvaJicvpfIg" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>

# Labeled Vs Unlabeled Data
On the other hand, it is also important to know and recognize another machine learning method that deals with unlabeled data. Some examples include images, audio recordings, and medical scan reports. If the data has no labels, how is it possible to fit an algorithm or a model onto the dataset? As we saw before, models are optimized to reduce error given a label. It is very much possible to do this task with the help of a method called unsupervised learning. Unsupervised learning uses machine learning algorithms to analyze and cluster unlabeled datasets. The purpose of this model is to discover any hidden trends or patterns, mainly similarities and differences among the dataset through clustering methods. Grouping the datasets into different clusters based on similar features can be done in a variety of methods. One way to use unsupervised learning is KMeans that separates a dataset into k clusters based on k centroids of data. The algorithm measures the average distance of every point from the centroids and groups the points closest to each centroid creating k clusters. 

Before we go onto testing an algorithm with a dataset, it is vital to know some data structures to use in python. The three most important libraries in python to use are Numpy's, Pandas, SciKit Learn, and Matplotlib. Matplotlib is a plotting library from Python to create visual representations of data sets in varieties of ways and structures. Pandas is a software library from Python used for the manipulation and arrangement of numerical data. Numpy is an open-source library from Python to generate multi-dimensional arrays and data structures. 

- <img src="IMG-1404.jpg" alt="Assembled Rover" width="500" height="400">
- <img src="IMG-1406.jpg" alt="Assembled Rover" width="500" height="400">
- <iframe width="560" height="315" src="https://www.youtube.com/embed/X3QUw-6nA50" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>

# Testing Models on California Housing & FIFA 2023 Cards Datasets
I initially trained some models on dataframes of California housing prices. These are the steps I took to training my models: 
I tried the model of linear regression by using  mean squared error between the housing values predicted by the regression line and the actual and observed housing values. 
These are the steps I took to training my models: 
1. First, my mentor and I made a regression line between x and y variables. 
2. We decided to make every feature (longitude, latitude, housing median age, total number of rooms, total number of bedrooms, population, households, and median income) the explanatory or independent variable while we made the median house value the response variable. 
3. From the regression line, we can understand the relationship between the changes in the x variables and the changes in housing values. 
4. We first made individual arrays for the x and y variables. 
5. Lastly, we made a function to estimate the coefficient of slope and the y-intercept for the regression line.

Then we went on to train and test models on another database called FIFA2023 card ratings for each individual player. I wanted to find patterns and relationships between physical and technical attributes (dribbling, pace, power, shooting power) and the player’s individual rating. For this starting experiment, I decided to make the physical and technical attributes as the x and explanatory variable and the player ratings as the y. I also have a CSV file of all fifa player ratings, and I converted all the files into a labeled data table using the library pandas as pd to read the CSV file.  Before training the models, I needed to define x and y-variables for my dataset. In this case, if I want to predict the rating of any fifa player when training the model, then I made pace, shooting, dribbling, passing, defending, and physical as my x variables to predict the players’ ratings as a the y. Because of this, I defined an array labeled x to locate any data for those attributes for any player in the x-labeled array. I made the array the fifa rating depending on the player. 

Like every model, I first train a portion of the data to see how well the model can fit that section of data and then seek validation from the results after training a model. After I train, I finally decide to test the same model on the remaining portion of the data. In this case, I split the quantitative data for FIFA cards into 80% for training the data and 20% for testing the data. This means that I kept a separate list of x data to train (x_train) and x data to test (x_test). 

In this case, since I was trying to make predictions for fifa ratings, I used the linear regression model to report the slope coefficient and the y-intercept of the regression line between the x and y data without considering the tested and trained data ( I would need this for later models). Later, I would implement K Nearest Neighbors for regression purposes. Before I explain the purpose of implementing this model, it is important to understand this supervised machine learning model. The k-nearest neighbors algorithm, also known as KNN or k-NN, is a non-parametric, supervised learning classifier, which uses proximity to make classifications or predictions about the grouping of an individual data point (“K-Nearest Neighbors Algorithm”). In general the KNN model uses an individual data point to find the k nearest points using euclidean distances of the x and y values. For regression, the KNN model memorizes the euclidean distances around the specific x data point on the cartesian plane and analyzes the average of the k nearest neighbors. This is used to make a prediction about the classification of the neighbors, and therefore K Nearest Neighbors uses the classification’s observations to come up with the best model between the x and y values. 

- <img src="IMG-1373.jpg" alt="Assembled Rover" width="500" height="400">
- In order to test my code, I would use an USB adapter to upload my code into the arduino board.
- <img src="IMG-1374.jpg" alt="" width="500" height="400">

#Amazon Customer Reviews Dataset

- <img src="Arduino-UNO-Description.png" alt="Arduino board" width="500" height="400">
- <img src="H-Bridge-Motor-Driver-Circuit-using-L293D.jpeg" alt="H Bridge" width="500" height="400">
<iframe width="560" height="315" src="https://www.youtube.com/embed/DG3XcSVxVLQ" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>

#Amazon Customer Reviews Dataset

# Schematics 
![Headstone Image](5A416919-024C-4747-82ED-2BB453C713FC.jpeg) 


